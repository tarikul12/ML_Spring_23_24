{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bebd609-1288-4fc1-bdb1-0443e7783add",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Multiple Regression Exercise\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy.linalg as la\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb1c521-4713-4f83-a47e-c5d63383a586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params GD:  [254449.99982048  78079.18106675  24442.5758378    2075.95636731]\n",
      "Params SGD:  [254298.87537186  78230.86686115  24961.15729194   3973.21648314]\n",
      "Training RMSE: 61070.62.\n",
      "Training cost: 1864810304.94.\n",
      "Test RMSE: 58473.59.\n",
      "Test cost: 1709580288.69.\n",
      "Training RMSE SGD: 61104.39.\n",
      "Training cost SGD: 1866873092.03.\n",
      "Test RMSE SGD: 58971.77.\n",
      "Test cost SGD: 1738834740.06.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Compute the sample mean and standard deviations for each feature (column)\n",
    "# across the training examples (rows) from the data matrix X.\n",
    "def mean_std(X):\n",
    "  mean = np.zeros(X.shape[1])\n",
    "  std = np.ones(X.shape[1])\n",
    "  ## Your code here. Hint: You can use numpy to compute mean and std.\n",
    "\n",
    "  mean = np.mean(X, axis=0)\n",
    "  std = np.std(X, axis=0)\n",
    "  return mean, std\n",
    "\n",
    "# Standardize the features of the examples in X by subtracting their mean and \n",
    "# dividing by their standard deviation, as provided in the parameters.\n",
    "def standardize(X, mean, std):\n",
    "  S = np.zeros(X.shape)\n",
    "\n",
    "  ## Your code here.\n",
    "  S = (X - mean) / std\n",
    "  return S\n",
    "\n",
    "# Read data matrix X and labels t from text file.\n",
    "def read_data(file_name):\n",
    "  data = np.loadtxt(file_name)\n",
    "  # Your code here. Load data features in X and labels in t.\n",
    "  X = data[:, :-1]\n",
    "  t = data[:, -1]\n",
    "  return X, t\n",
    "\n",
    "# Implement gradient descent algorithm to compute w = [w0, w1, ..].\n",
    "def train(X, t, eta, epochs):\n",
    "  #  YOUR CODE here:\n",
    "  costs = []\n",
    "  ep = []\n",
    "  w = np.zeros(X.shape[1])\n",
    "  \n",
    "  #  YOUR CODE here. Implement gradient descent to compute w for given epochs.\n",
    "  #  Use 'compute_gradient' function below to find gradient of cost function and update w each epoch.\n",
    "  #  Compute and append cost and epoch number to variables costs and ep every 10 epochs.\n",
    " \n",
    "  for epoch in range(epochs):\n",
    "    gradient = compute_gradient(X, t, w)\n",
    "    w -= eta * gradient\n",
    "    if epoch % 10 == 0:\n",
    "      costs.append(compute_cost(X, t, w))\n",
    "      ep.append(epoch)\n",
    "  return w, ep, costs\n",
    "\n",
    "# Compute RMSE on dataset (X, t).\n",
    "def compute_rmse(X, t, w):\n",
    "  #  YOUR CODE here:\n",
    "  predictions = np.dot(X, w)\n",
    "  rmse = np.sqrt(np.mean((predictions - t)**2))\n",
    "  return rmse\n",
    "\n",
    "# Compute objective function (cost) on dataset (X, t).\n",
    "def compute_cost(X, t, w):\n",
    "  #  YOUR CODE here:\n",
    "  predictions = np.dot(X, w)\n",
    "  cost = np.mean((predictions - t)**2) / 2\n",
    "  return cost\n",
    "\n",
    "# Compute gradient of the objective function (cost) on dataset (X, t).\n",
    "def compute_gradient(X, t, w):\n",
    "  #  YOUR CODE here:\n",
    "  grad = np.zeros(w.shape)\n",
    "  grad = np.dot(X.T, (np.dot(X, w) - t)) / X.shape[0]\n",
    "  \n",
    "  return grad\n",
    "\n",
    "# BONUS: Implement stochastic gradient descent algorithm to compute w = [w0, w1, ..].\n",
    "def train_SGD(X, t, eta, epochs):\n",
    "  #  YOUR CODE here:\n",
    "  costs = []\n",
    "  ep = []\n",
    "  w = np.zeros(X.shape[1])\n",
    "  #  YOUR CODE here. Implement stochastic gradient descent to compute w for given epochs. \n",
    "  #  Compute and append cost and epoch number to variables costs and ep every 10 epochs.\n",
    "  for epoch in range(epochs):\n",
    "    for i in range(X.shape[0]):\n",
    "      rand_index = np.random.randint(X.shape[0])\n",
    "      xi = X[rand_index:rand_index+1]\n",
    "      ti = t[rand_index:rand_index+1]\n",
    "      gradient = compute_gradient(xi, ti, w)\n",
    "      w -= eta * gradient/ X.shape[0]\n",
    "    if epoch % 10 == 0:\n",
    "      costs.append(compute_cost(X, t, w))\n",
    "      ep.append(epoch)\n",
    "  return w, ep, costs\n",
    "\n",
    "##======================= Main program =======================##\n",
    "parser = argparse.ArgumentParser('Multiple Regression Exercise.')\n",
    "parser.add_argument('-i', '--input_data_dir',\n",
    "                    type=str,\n",
    "                    default='linear_regression/data/multiple',\n",
    "                    help='Directory for the multiple regression houses dataset.')\n",
    "FLAGS, unparsed = parser.parse_known_args()\n",
    "\n",
    "# Read the training and test data.\n",
    "Xtrain, ttrain = read_data(FLAGS.input_data_dir + \"/train.txt\")\n",
    "Xtest, ttest = read_data(FLAGS.input_data_dir + \"/test.txt\")\n",
    "\n",
    "#  YOUR CODE here: \n",
    "#  Standardize the training and test features using the mean and std computed over *training*.\n",
    "#  Make sure you add the bias feature to each training and test example.\n",
    "#  The bias features should be a column of ones addede as the first columns of training and test examples\n",
    "\n",
    "# Compute mean and standard deviation for training data.\n",
    "mean, std = mean_std(Xtrain)\n",
    "\n",
    "# Standardize the training and test features using the mean and std computed over *training*.\n",
    "Xtrain_std = np.hstack((np.ones((Xtrain.shape[0], 1)), standardize(Xtrain, mean, std)))\n",
    "Xtest_std = np.hstack((np.ones((Xtest.shape[0], 1)), standardize(Xtest, mean, std)))\n",
    "\n",
    "# Computing parameters for each training method for eta=0.1 and 200 epochs\n",
    "eta = 0.1\n",
    "epochs = 200\n",
    "w, eph, costs = train(Xtrain_std, ttrain, eta, epochs)\n",
    "wsgd, ephsgd, costssgd = train_SGD(Xtrain_std, ttrain, eta, epochs)\n",
    "\n",
    "# Print model parameters.\n",
    "print('Params GD: ', w)\n",
    "print('Params SGD: ', wsgd)\n",
    "\n",
    "# Print cost and RMSE on training data.\n",
    "print('Training RMSE: %0.2f.' % compute_rmse(Xtrain_std, ttrain, w))\n",
    "print('Training cost: %0.2f.' % compute_cost(Xtrain_std, ttrain, w))\n",
    "\n",
    "# Print cost and RMSE on test data.\n",
    "print('Test RMSE: %0.2f.' % compute_rmse(Xtest_std, ttest, w))\n",
    "print('Test cost: %0.2f.' % compute_cost(Xtest_std, ttest, w))\n",
    "\n",
    "\n",
    "# Compute RMSE and cost for SGD on training data.\n",
    "print('Training RMSE SGD: %0.2f.' % compute_rmse(Xtrain_std, ttrain, wsgd))\n",
    "print('Training cost SGD: %0.2f.' % compute_cost(Xtrain_std, ttrain, wsgd))\n",
    "\n",
    "# Compute RMSE and cost for SGD on test data.\n",
    "print('Test RMSE SGD: %0.2f.' % compute_rmse(Xtest_std, ttest, wsgd))\n",
    "print('Test cost SGD: %0.2f.' % compute_cost(Xtest_std, ttest, wsgd))\n",
    "\n",
    "\n",
    "# Plotting Epochs vs. cost for Gradient descent methods\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('cost')\n",
    "plt.yscale('log')\n",
    "plt.plot(eph, costs, 'bo-', label='train_Jw_gd')\n",
    "plt.plot(ephsgd, costssgd, 'ro-', label='train_Jw_sgd')\n",
    "plt.legend()\n",
    "plt.savefig('gd_cost_multiple.png')\n",
    "plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
