{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tarikul12/ML_Spring_23_24/blob/main/value_iteration.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f952b1-f2c3-4976-a3be-dbd07e4e1808",
      "metadata": {
        "id": "11f952b1-f2c3-4976-a3be-dbd07e4e1808"
      },
      "source": [
        "## Value Iteration Exercise\n",
        "\n",
        "**Solve the following MDP problem using value iteration:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e4db1479-d163-476e-9140-75cafce8c858",
      "metadata": {
        "id": "e4db1479-d163-476e-9140-75cafce8c858"
      },
      "source": [
        "A robot performs picking and packaging task in a warehouse, while “picking” is to gather required items from different locations of the warehouse and “packaging” is to bring those items to a specific location where they can be packaged. In this problem, the robot will need to find out the shortest path between picking location and packaging location through different algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ff07cd1-aabe-4d23-9194-26b0129cf261",
      "metadata": {
        "id": "3ff07cd1-aabe-4d23-9194-26b0129cf261"
      },
      "source": [
        "![image.png](attachment:571de4ad-f712-433b-a197-789689ceb4a6.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a8c58c8-dd4e-4dbc-81f5-e327dc06d336",
      "metadata": {
        "id": "4a8c58c8-dd4e-4dbc-81f5-e327dc06d336"
      },
      "source": [
        "This is a grid world problem and its environment includes states, actions and rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d7a3bd-6c76-479f-814e-8a6b96baeb3c",
      "metadata": {
        "id": "b6d7a3bd-6c76-479f-814e-8a6b96baeb3c"
      },
      "source": [
        "**State:** All possible locations within warehouse. Some locations are storing shelves (black squares), and other locations are aisles that are picking locations and can be used to travel; there is one location (green square) and it represents the item packaging location, the goal of the robot. There are total of 11 x 11 = 121 possible states, and each state can be identified by its row and column index. The storing shelves and packaging location are terminal states.\n",
        "\n",
        "**Actions:** Available direction that a robot agent can move, it consists of Up, Right, Down and Left. A robot must learn to avoid hitting the storage locations.\n",
        "\n",
        "**Rewards:** Each state used for travelling is assigned a small punishment of 1 (reward of -1), and each state of the storage location is assigned a big punishment of 100 (reward of -100). Finally, the packaging location is assigned a big reward value of 100. A robot must learn to travel through the shortest path and avoid driving into storing shelves in order to minimize punishment, ie. maximize rewards."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22cd8842-b173-4535-8a99-5bd91db46b39",
      "metadata": {
        "id": "22cd8842-b173-4535-8a99-5bd91db46b39"
      },
      "source": [
        "![image.png](attachment:f9935d85-c6ac-40f7-a879-5eb76a11cb6b.png)\n",
        "\n",
        "                   Reward Assigned to Each State"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5735c500-4442-4f8b-8019-bd619519d10c",
      "metadata": {
        "id": "5735c500-4442-4f8b-8019-bd619519d10c"
      },
      "source": [
        "In value iteration algorithm, we can set the convergence tolerance to 0.01 for each state, ie. stop looping when the utility value difference between current iteration and previous iteration is less than 0.01 at each state. It took 27 iterations to converge and the result returned the direction at each state a robot should head to. Below figure shows the optimal policy output from the program, where at each aisle location, 0, 1, 2 and 3 denotes to move up, right, down and left, respectively. It was then converted to a clearer policy map:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 503,
      "id": "e617778d-5b1b-44fc-bd47-f14b45038d16",
      "metadata": {
        "id": "e617778d-5b1b-44fc-bd47-f14b45038d16"
      },
      "outputs": [],
      "source": [
        "#([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        "# [0. 1. 1. 1. 1. 0. 3. 3. 3. 3. 0.]\n",
        " #[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        " #[0. 0. 3. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
        " #[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        " #[1. 1. 1. 1. 1. 1. 1. 0. 3. 3. 3.]\n",
        " #[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        " #[0. 1. 1. 1. 1. 0. 3. 3. 3. 3. 0.]\n",
        " #[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
        " #[1. 1. 1. 0. 3. 1. 1. 0. 3. 3. 3.]\n",
        " #[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53efb549-c7be-4825-89cb-b76f75580b92",
      "metadata": {
        "id": "53efb549-c7be-4825-89cb-b76f75580b92"
      },
      "source": [
        "![image.png](attachment:b2310fe7-9afb-42c7-920b-104812bc6679.png)  \n",
        "                        \n",
        "                         Policy Map (Direction Map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c72f72-a8d5-4aad-b333-4e0c25675ccd",
      "metadata": {
        "id": "11c72f72-a8d5-4aad-b333-4e0c25675ccd"
      },
      "source": [
        "The key equation to reinforcement is Bellman Equation and it fully encodes the utility/value of a state in a Markov Decision Process. Value iteration and policy iteration are the two basic methods of solving Bellman Equation and thus computing an optimal Markov Decision Process. Here we will use value iteration to solve an MDP problem — warehouse problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16cabd02-4d43-4993-a7b8-faa50ae736f4",
      "metadata": {
        "id": "16cabd02-4d43-4993-a7b8-faa50ae736f4"
      },
      "source": [
        "![image.png](attachment:7575f452-ef85-499e-9b30-d694a3d8c2b6.png)\n",
        "\n",
        "        Bellman equation\n",
        "\n",
        "![image.png](attachment:1a5ba2e0-f45c-4e9d-966b-fbb454eb68d2.png)\n",
        "\n",
        "    Equation in Value Iteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 504,
      "id": "e37c6fc3-484e-4c4b-84c5-26f4d6937971",
      "metadata": {
        "id": "e37c6fc3-484e-4c4b-84c5-26f4d6937971"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "environment_rows = 11\n",
        "environment_columns = 11\n",
        "actions = ['up', 'right', 'down', 'left']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 505,
      "id": "a7ad6bb1-a01e-4c27-9ce9-451050458e51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7ad6bb1-a01e-4c27-9ce9-451050458e51",
        "outputId": "959fb09c-b4c4-4014-bf7f-cd00b38511c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rewards for each state:\n",
            "[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            "[-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
            "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
            "[-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            "[-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
            "[-1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.]\n",
            "[-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]\n"
          ]
        }
      ],
      "source": [
        "# Create 2D array to hold rewards for each state\n",
        "\n",
        "rewards = np.full((environment_rows, environment_columns), -100.)\n",
        "rewards[0, 5] = 100.\n",
        "\n",
        "aisles = {}\n",
        "aisles[1] = [i for i in range(1, 10)]\n",
        "aisles[2] = [1, 7, 9]\n",
        "aisles[3] = [i for i in range(1, 8)]\n",
        "aisles[3].append(9)\n",
        "aisles[4] = [3, 7]\n",
        "aisles[5] = [i for i in range(11)]\n",
        "aisles[6] = [5]\n",
        "aisles[7] = [i for i in range(1, 10)]\n",
        "aisles[8] = [3, 7]\n",
        "aisles[9] = [i for i in range(11)]\n",
        "\n",
        "for row_index in range(1, 10):\n",
        "    for column_index in aisles[row_index]:\n",
        "        rewards[row_index, column_index] = -1\n",
        "\n",
        "print('Rewards for each state:')\n",
        "for row in rewards:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 506,
      "id": "42b60158-6508-47d2-a4aa-7bdc8ffec22a",
      "metadata": {
        "id": "42b60158-6508-47d2-a4aa-7bdc8ffec22a"
      },
      "outputs": [],
      "source": [
        "# initialize V value at each state\n",
        "V = np.zeros((environment_rows, environment_columns,1))\n",
        "V [:,:,0] = rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 507,
      "id": "e0248335-906f-4f88-8afb-910138af2a76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0248335-906f-4f88-8afb-910138af2a76",
        "outputId": "4e772dad-2e3a-4d2a-f2b1-76babd70ebeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-100. -100. -100. -100. -100.  100. -100. -100. -100. -100. -100.]\n",
            " [-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            " [-100.   -1. -100. -100. -100. -100. -100.   -1. -100.   -1. -100.]\n",
            " [-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.   -1. -100.]\n",
            " [-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [-100. -100. -100. -100. -100.   -1. -100. -100. -100. -100. -100.]\n",
            " [-100.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1. -100.]\n",
            " [-100. -100. -100.   -1. -100. -100. -100.   -1. -100. -100. -100.]\n",
            " [  -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.   -1.]\n",
            " [-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.]]\n"
          ]
        }
      ],
      "source": [
        "# Print initialized values V\n",
        "print(V[:,:,0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 508,
      "id": "938a8d28-aa47-439c-85c3-27dfbd105a2d",
      "metadata": {
        "id": "938a8d28-aa47-439c-85c3-27dfbd105a2d"
      },
      "outputs": [],
      "source": [
        "# initialize policy at each state\n",
        "optimal_policy = np.zeros((environment_rows, environment_columns,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 509,
      "id": "32cb26c0-f011-4b69-b472-c703314c8548",
      "metadata": {
        "id": "32cb26c0-f011-4b69-b472-c703314c8548"
      },
      "outputs": [],
      "source": [
        "#define a function that determines if the specified location is a terminal state\n",
        "def is_terminal_state(current_row_index, current_column_index):\n",
        "    if rewards[current_row_index, current_column_index] == -1.:\n",
        "        return False\n",
        "    else:\n",
        "        return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 510,
      "id": "bd1243a5-bcf4-496c-bb09-e9c2ca27ba49",
      "metadata": {
        "id": "bd1243a5-bcf4-496c-bb09-e9c2ca27ba49"
      },
      "outputs": [],
      "source": [
        "#define a function that will get the next location based on the chosen action\n",
        "def get_next_location(current_row_index, current_column_index, action_index):\n",
        "    new_row_index = current_row_index\n",
        "    new_column_index = current_column_index\n",
        "    if actions[action_index] == 'up' and current_row_index > 0:\n",
        "        new_row_index -= 1\n",
        "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
        "        new_column_index += 1\n",
        "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
        "        new_row_index += 1\n",
        "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
        "        new_column_index -= 1\n",
        "    return new_row_index, new_column_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 511,
      "id": "3edcdf0c-cbfa-4be0-8f45-a6a20251acce",
      "metadata": {
        "id": "3edcdf0c-cbfa-4be0-8f45-a6a20251acce"
      },
      "outputs": [],
      "source": [
        "epsilon = 0.9 # percentage of time to take the best action (instead of a random action)\n",
        "pr_best = epsilon + (1-epsilon)/4 # probability of moving to the best adjacent location\n",
        "pr_others = (1-epsilon)/4 # probability of moving to the adjacent locations other than the best\n",
        "discount_factor = 0.9 # discount factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 512,
      "id": "2f52ffe6-c1da-4256-a544-2bb3f0663572",
      "metadata": {
        "id": "2f52ffe6-c1da-4256-a544-2bb3f0663572"
      },
      "outputs": [],
      "source": [
        "def value_iterations(V):\n",
        "    number_iterations = 0\n",
        "    while True:\n",
        "        # Store current values of states\n",
        "        oldV = V.copy()\n",
        "        # Iterate and update value and optimal policy of each state\n",
        "        for row_index in range(0,11):\n",
        "            for column_index in range(0,11):\n",
        "                if not is_terminal_state(row_index, column_index):\n",
        "                    Q = {}\n",
        "                    # Find value of taking each action a in current state s = (row_index, column_index) and store it in Q\n",
        "                    for a in range(0,4):\n",
        "                        new_row_index, new_column_index = get_next_location(row_index, column_index, a)\n",
        "\n",
        "                        # Find out the other 3 non-favorable directions\n",
        "                        direction_index = np.array([0,1,2,3])\n",
        "                        other_direction_index = np.delete(direction_index, np.where(direction_index == a))\n",
        "                        other_row_index_1, other_column_index_1 = get_next_location(row_index, column_index, other_direction_index[0])\n",
        "                        other_row_index_2, other_column_index_2 = get_next_location(row_index, column_index, other_direction_index[1])\n",
        "                        other_row_index_3, other_column_index_3 = get_next_location(row_index, column_index, other_direction_index[2])\n",
        "\n",
        "                        # Compute Q[a] for the current state s = (row_index, column_index) and action a\n",
        "                        # Q[a] = R[s] + gamma*(ΣP(s,a,s')*V(s'))\n",
        "                        # R[s] is the reward at current state s\n",
        "                        # P(s,a,s') is the probability of moving to new state s' = (row_index',column_index') given state s and action a\n",
        "                        # V(s') is the current value of new state s'. Here V(s') is given by oldV[row_index',column_index',0]\n",
        "                        Q[a] = rewards[new_row_index, new_column_index] + discount_factor * (\n",
        "                            0.8 * oldV[new_row_index, new_column_index, 0] +\n",
        "                            0.1 * oldV[other_row_index_1, other_column_index_1, 0] +\n",
        "                            0.1 * oldV[other_row_index_2, other_column_index_2, 0] +\n",
        "                            0.1 * oldV[other_row_index_3, other_column_index_3, 0]\n",
        "                        )\n",
        "\n",
        "                    # Update V(s) with the optimal value, given by the maximum value stored in Q where s is current state\n",
        "                    # Update optimal_policy(s) with the optimal action, given by the index of the maximum value stored in Q\n",
        "                    V[row_index, column_index, 0] = max(Q.values())\n",
        "                    optimal_policy[row_index, column_index, 0] = max(Q, key=Q.get)\n",
        "\n",
        "        number_iterations += 1\n",
        "        if (abs(oldV[:,:,0] - V[:,:,0]) < 0.3).all():\n",
        "            break\n",
        "    return V, optimal_policy, number_iterations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 513,
      "id": "3d59a793-6649-4234-8b29-030c75adf4a8",
      "metadata": {
        "id": "3d59a793-6649-4234-8b29-030c75adf4a8"
      },
      "outputs": [],
      "source": [
        "# Obtain optimal value and policy\n",
        "V, optimal_policy, number_iterations = value_iterations(V)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 514,
      "id": "ab183cb2-c779-47c8-8444-c0abae43b64a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab183cb2-c779-47c8-8444-c0abae43b64a",
        "outputId": "1ec7102e-c5d2-464c-e54f-45c576d3f613"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy:\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 1. 0. 3. 3. 3. 3. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 3. 1. 1. 1. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 1. 1. 1. 1. 0. 3. 3. 3.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 1. 1. 1. 0. 3. 3. 3. 3. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 1. 1. 0. 3. 1. 1. 0. 3. 3. 3.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Iterations:\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# Print optimal policy and number of iterations\n",
        "optimal_policy = optimal_policy[:,:,0]\n",
        "print('Optimal policy:')\n",
        "print(optimal_policy)\n",
        "print('Iterations:')\n",
        "print(number_iterations)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}